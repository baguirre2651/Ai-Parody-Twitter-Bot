{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python\n",
    "import pandas as pd \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YourKey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Bryan Aguirre Twitter Parody \n",
    "\n",
    "\n",
    "For this project I thought it would be fun to create a parody account mimicking and predicting funny and sarcastic tweets from myself. \n",
    "\n",
    "\n",
    "In this notebook we'll look at the prompting technique I used. \n",
    "\n",
    "\n",
    "and in the full code you'll see the other helper code used to deploy this app.\n",
    "\n",
    "# Here's the Gist of how it will work:\n",
    "\n",
    "A user @mentions your bot, for me it will be @b_ryanaguirre_ or Bryan Aguirre (Parody)\n",
    "\n",
    "The script starts by finding a new @mention and then reads the parent tweet it is being mentioned on\n",
    "\n",
    "The script will then take that parent tweet and generates a witty response using a language model\n",
    "\n",
    "Respond is posted and tweet is logged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we train our model, \n",
    "\n",
    "we start with creating a function that will take in/feed in my tweet, and give an output response.\n",
    "\n",
    "we are making sure it is under 200 characters and creating around a 5-10 min response window given the norm behind twitter limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.3,\n",
    "                 openai_api_key=OPENAI_API_KEY,\n",
    "#                  model_name='gpt-3.5-turbo',\n",
    "                 model_name='gpt-4',\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Editing Process\n",
    "\n",
    "\n",
    "Prompt \n",
    "\n",
    "You are an incredibly wise, and smart, sarcastic, comedian. Your goal is to give a concise prediction in response to a piece of text from Bryan Aguirre.\n",
    "\n",
    "\n",
    "% RESPONSE TONE:\n",
    "\n",
    "Your prediction should be given in an active voice and be opinionated\n",
    "Your tone should be serious w/ a hint of wit and sarcasm\n",
    "\n",
    "% RESPONSE FORMAT:\n",
    "\n",
    "Respond in under 200 characters\n",
    "Respond in one short sentence\n",
    "Do not respond with emojis\n",
    "\n",
    "% RESPONSE CONTENT:\n",
    "\n",
    "Include specific examples if they are relevant\n",
    "If you don't have an answer, say, \"Sorry, nevermind \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(llm, mentioned_parent_tweet_text):\n",
    "    # It would be nice to bring in information about the links, pictures, etc.\n",
    "    # But out of scope for now\n",
    "    \n",
    "    system_template = \"\"\"\n",
    "        You are an incredibly wise and smart tech mad scientist from silicon valley.\n",
    "        Your goal is to give a concise prediction in response to a piece of text from the user.\n",
    "        \n",
    "        % RESPONSE TONE:\n",
    "\n",
    "        - Your prediction should be given in an active voice and be opinionated\n",
    "        - Your tone should be serious w/ a hint of wit and sarcasm\n",
    "        \n",
    "        % RESPONSE FORMAT:\n",
    "\n",
    "        - Respond in under 200 characters\n",
    "        - Respond in two or less short sentences\n",
    "        - Do not respond with emojis\n",
    "        \n",
    "        % RESPONSE CONTENT:\n",
    "\n",
    "        - Include specific examples of old tech if they are relevant\n",
    "        - If you don't have an answer, say, \"Sorry, my magic 8 ball isn't working right now ðŸ”®\"\n",
    "    \"\"\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "    human_template=\"{text}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "    # get a chat completion from the formatted messages\n",
    "    final_prompt = chat_prompt.format_prompt(text=mentioned_parent_tweet_text).to_messages()\n",
    "    response = llm(final_prompt).content\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"\n",
    "I wanted to build a sassy Twitter Bot Bryan Aguirre (Parody) that responds, parodies, and predicts tweets by Bryan Aguirre \n",
    "\n",
    "@b_ryanaguirre_ was built using @LangChainAI and hosted on @railway \n",
    "\n",
    "Condensed Prompt:\n",
    "You are an incredibly wise, and smart, sarcastic, comedian. Your goal is to give a concise prediction in response to a piece of text from Bryan.\n",
    "\"\"\"\n",
    "\n",
    "response = generate_response(llm, tweet)\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
